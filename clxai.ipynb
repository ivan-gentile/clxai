{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "T6KRiDQBhHs8",
        "elYjIz5WmXp4",
        "JdJnPgawoEmE",
        "nhpgTi_UoH5y",
        "l0Kp8HQTpBYS",
        "siS_uxP7pFNL",
        "Ktt1wxFm40_3",
        "Rxe-0gywsQFQ",
        "_9j4Yjy8sTKL"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "6NXrXuhyjfa3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0K32a0SZjXvF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "913aafee-60fc-43c2-a874-a8596cdd579f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'clxai' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/LeonardoArrighi/clxai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "from clxai.src.models.resnet import get_model\n",
        "from clxai.src.utils.data import get_data_loaders, get_num_classes"
      ],
      "metadata": {
        "id": "HAyLkPYjj0xD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tD87neF7k30-",
        "outputId": "1bd0e28e-ade9-4865-ff19-84ca280422a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0beZ2goKq7T",
        "outputId": "0ef675ac-4dc6-4e2a-dbdf-354c26f284b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_version = 'scl' # THIS\n",
        "train_flag = False\n",
        "test_flag = False"
      ],
      "metadata": {
        "id": "PtTSccSGhARS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if model_version == 'ce':\n",
        "  model_path = '/content/drive/MyDrive/clxai/weights/ce_seed0_best_model.pt'\n",
        "elif model_version == 'scl' and train_flag:\n",
        "  model_path = '/content/drive/MyDrive/clxai/weights/scl_seed1_best_model.pt'\n",
        "  weight_path = '/content/drive/MyDrive/clxai/scl_mlp/scl_seed1_model_mlp.pt'\n",
        "elif model_version == 'scl' and not train_flag:\n",
        "  model_path = '/content/drive/MyDrive/clxai/scl_mlp/scl_seed1_model_mlp.pt'"
      ],
      "metadata": {
        "id": "ukANyL7-73Wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# res_path = '/content/drive/MyDrive/clxai/results_faith/test'"
      ],
      "metadata": {
        "id": "3-epkA5QLUBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = 'cifar10'\n",
        "architecture = 'resnet18'\n",
        "num_classes = get_num_classes(dataset)"
      ],
      "metadata": {
        "id": "tdESFt_LhbGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, test_loader = get_data_loaders(\n",
        "        dataset=dataset,\n",
        "        # data_dir = ,\n",
        "        batch_size = 128,\n",
        "        num_workers = 2,\n",
        "        augment = False,\n",
        "        download = True # riga 340 in data\n",
        ")"
      ],
      "metadata": {
        "id": "yncJSs9KhgjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "BP8PI-6_hlcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model(architecture = architecture,\n",
        "                  num_classes = num_classes)\n",
        "\n",
        "if model_version == 'ce':\n",
        "\n",
        "  checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
        "\n",
        "  loaded_state_dict = checkpoint['model_state_dict']\n",
        "\n",
        "  model.load_state_dict(loaded_state_dict, strict=False)\n",
        "\n",
        "\n",
        "if model_version == 'scl' and train_flag:\n",
        "\n",
        "  checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
        "\n",
        "  loaded_state_dict = checkpoint['model_state_dict']\n",
        "\n",
        "  model.load_state_dict(loaded_state_dict, strict=False)\n",
        "\n",
        "\n",
        "  new_state_dict = {}\n",
        "  for k, v in loaded_state_dict.items():\n",
        "      if k.startswith('fc.'):\n",
        "          continue\n",
        "      else:\n",
        "          new_state_dict['encoder.' + k] = v\n",
        "\n",
        "  # Dopo model.load_state_dict(new_state_dict, strict=False)\n",
        "  loaded_keys = set(new_state_dict.keys())\n",
        "  model_keys = set(model.state_dict().keys())\n",
        "  matched_keys = loaded_keys.intersection(model_keys)\n",
        "\n",
        "  print(f\"Parametri caricati con successo: {len(matched_keys)}\")\n",
        "  print(f\"Parametri mancanti: {len(model_keys - matched_keys)}\")\n",
        "\n",
        "  if len(matched_keys) == 0:\n",
        "      print(\"ATTENZIONE: Nessuna chiave corrisponde!\")\n",
        "\n",
        "  model.load_state_dict(new_state_dict, strict=False)\n",
        "\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNmArVbX6Hcc",
        "outputId": "c2137858-f017-4afc-bff2-56fd8e223c77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet18(\n",
              "  (encoder): ResNet18Encoder(\n",
              "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (projection): Identity()\n",
              "  )\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Per CL training"
      ],
      "metadata": {
        "id": "0pNWP58sZj7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# modifichiamo il classificatore e lo congeliamo\n",
        "\n",
        "if model_version == 'scl' and train_flag:\n",
        "\n",
        "  import torch.nn as nn\n",
        "\n",
        "  in_features = model.fc.in_features\n",
        "  hidden_dim = 256\n",
        "\n",
        "  model.fc = nn.Sequential(\n",
        "      nn.Linear(in_features, hidden_dim),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(0.2),\n",
        "      nn.Linear(hidden_dim, num_classes)\n",
        "  )\n",
        "\n",
        "  # freeze\n",
        "  for name, param in model.named_parameters():\n",
        "      if \"fc\" not in name:\n",
        "          param.requires_grad = False\n",
        "\n",
        "  model.to(device)"
      ],
      "metadata": {
        "id": "bPtyU_gy2V3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# addestriamo il MLP finale\n",
        "if model_version == 'scl' and train_flag:\n",
        "\n",
        "  import torch.optim as optim\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.fc.parameters(), lr=1e-3)\n",
        "\n",
        "  num_epochs = 20\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "      model.train()\n",
        "      running_loss = 0.0\n",
        "      correct = 0\n",
        "      total = 0\n",
        "\n",
        "      for inputs, labels in train_loader:\n",
        "          inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, labels)\n",
        "\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          running_loss += loss.item()\n",
        "          _, predicted = outputs.max(1)\n",
        "          total += labels.size(0)\n",
        "          correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "      print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {running_loss/len(train_loader):.4f} - Acc: {100.*correct/total:.2f}%\")\n",
        "\n",
        "      torch.save(model.state_dict(), weight_path)"
      ],
      "metadata": {
        "id": "uQmqjrHG246F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if model_version == 'scl' and not train_flag:\n",
        "  model = get_model(architecture=architecture, num_classes=num_classes)\n",
        "  in_features = model.fc.in_features\n",
        "  hidden_dim = 256\n",
        "\n",
        "  model.fc = nn.Sequential(\n",
        "      nn.Linear(in_features, hidden_dim),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(0.2),\n",
        "      nn.Linear(hidden_dim, num_classes)\n",
        "  )\n",
        "\n",
        "  loaded_weights = torch.load(model_path, map_location=device, weights_only=False)\n",
        "  model.load_state_dict(loaded_weights, strict=False)\n",
        "  model.to(device)\n",
        "  model.eval()"
      ],
      "metadata": {
        "id": "vXS9mGv0CBfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XAI"
      ],
      "metadata": {
        "id": "3NbjowRa3MLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install grad-cam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbsosOwz3Z24",
        "outputId": "ad263242-d39d-4171-ff62-76aa1098f269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: grad-cam in /usr/local/lib/python3.12/dist-packages (1.5.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from grad-cam) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from grad-cam) (11.3.0)\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.12/dist-packages (from grad-cam) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.12/dist-packages (from grad-cam) (0.24.0+cu126)\n",
            "Requirement already satisfied: ttach in /usr/local/lib/python3.12/dist-packages (from grad-cam) (0.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from grad-cam) (4.67.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from grad-cam) (4.13.0.90)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from grad-cam) (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from grad-cam) (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->grad-cam) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->grad-cam) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->grad-cam) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->grad-cam) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->grad-cam) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->grad-cam) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->grad-cam) (2.9.0.post0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->grad-cam) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->grad-cam) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->grad-cam) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->grad-cam) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.7.1->grad-cam) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.7.1->grad-cam) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_grad_cam import GradCAM, EigenCAM, HiResCAM, AblationCAM\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image"
      ],
      "metadata": {
        "id": "0fYOSBSO3X5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantus"
      ],
      "metadata": {
        "id": "veL-BSqPg1iE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install quantus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bGdR_3Dg3tU",
        "outputId": "34909a0e-7864-472d-e189-0979cc5ff22f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: quantus in /usr/local/lib/python3.12/dist-packages (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from quantus) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.5.3 in /usr/local/lib/python3.12/dist-packages (from quantus) (2.2.2)\n",
            "Requirement already satisfied: opencv-python>=4.5.5.62 in /usr/local/lib/python3.12/dist-packages (from quantus) (4.13.0.90)\n",
            "Requirement already satisfied: scikit-image>=0.19.3 in /usr/local/lib/python3.12/dist-packages (from quantus) (0.25.2)\n",
            "Requirement already satisfied: scikit-learn>=0.24.2 in /usr/local/lib/python3.12/dist-packages (from quantus) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.12/dist-packages (from quantus) (1.16.3)\n",
            "Requirement already satisfied: tqdm>=4.62.3 in /usr/local/lib/python3.12/dist-packages (from quantus) (4.67.1)\n",
            "Requirement already satisfied: matplotlib>=3.3.4 in /usr/local/lib/python3.12/dist-packages (from quantus) (3.10.0)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from quantus) (6.2.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.4->quantus) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.4->quantus) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.4->quantus) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.4->quantus) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.4->quantus) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.4->quantus) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.4->quantus) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.4->quantus) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.3->quantus) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.3->quantus) (2025.3)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.19.3->quantus) (3.6.1)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.19.3->quantus) (2.37.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.19.3->quantus) (2026.1.14)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.19.3->quantus) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.2->quantus) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.2->quantus) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.4->quantus) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import quantus\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "import matplotlib\n",
        "matplotlib.use('Agg') # Force non-interactive backend (prevents hanging)"
      ],
      "metadata": {
        "id": "PDGxT-5UhmNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "unfrozen_model = copy.deepcopy(model)\n",
        "\n",
        "for param in unfrozen_model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "unfrozen_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3By-aiKqZu3a",
        "outputId": "8b997027-0306-449d-9dc7-f7e95963f964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet18(\n",
              "  (encoder): ResNet18Encoder(\n",
              "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (projection): Identity()\n",
              "  )\n",
              "  (fc): Sequential(\n",
              "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.2, inplace=False)\n",
              "    (3): Linear(in_features=256, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pixel flipping"
      ],
      "metadata": {
        "id": "T6KRiDQBhHs8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def xai_scores_pf(model, test_loader, device, res_path=\"results\", plot_flag=False):\n",
        "    model.eval()\n",
        "    os.makedirs(res_path, exist_ok=True)\n",
        "    target_layers = [model.encoder.layer4[-1]]\n",
        "\n",
        "    pixel_flipping_metric = quantus.PixelFlipping(\n",
        "        perturb_baseline=\"black\",\n",
        "        features_in_step=32,\n",
        "        disable_warnings=True,\n",
        "    )\n",
        "\n",
        "    all_scores = []\n",
        "    global_idx = 0\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(test_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            preds = model(images).argmax(dim=1)\n",
        "\n",
        "        for i in range(images.size(0)):\n",
        "            p_idx = preds[i].item()\n",
        "            g_idx = labels[i].item()\n",
        "            img_tensor = images[i:i+1]\n",
        "\n",
        "            record = {\"idx\": global_idx, \"true\": g_idx, \"pred\": p_idx}\n",
        "\n",
        "            # Re-initialize methods inside or use a very tight loop to ensure cleanup\n",
        "            # We define them here so they don't carry state between samples\n",
        "            curr_methods = {\n",
        "                \"GradCAM\": GradCAM(model=model, target_layers=target_layers),\n",
        "                \"EigenCAM\": EigenCAM(model=model, target_layers=target_layers),\n",
        "                \"AblationCAM\": AblationCAM(model=model, target_layers=target_layers)\n",
        "            }\n",
        "\n",
        "            if plot_flag:\n",
        "                fig, axes = plt.subplots(1, 4, figsize=(18, 5))\n",
        "                img_np = images[i].permute(1, 2, 0).cpu().numpy()\n",
        "                img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min() + 1e-8)\n",
        "                axes[0].imshow(img_np)\n",
        "                axes[0].axis('off')\n",
        "\n",
        "            for m_idx, (name, cam_obj) in enumerate(curr_methods.items()):\n",
        "                # Tight gradient control\n",
        "                with torch.enable_grad():\n",
        "                    grayscale_cam = cam_obj(input_tensor=img_tensor,\n",
        "                                           targets=[ClassifierOutputTarget(p_idx)])[0, :]\n",
        "\n",
        "                # Metric calculation\n",
        "                # We do this on CPU to keep the GPU purely for the model\n",
        "                pf_score = pixel_flipping_metric(\n",
        "                    model=model,\n",
        "                    x_batch=img_tensor.detach().cpu().numpy(),\n",
        "                    y_batch=np.array([g_idx]),\n",
        "                    a_batch=grayscale_cam[np.newaxis, ...],\n",
        "                    device=device\n",
        "                )[0]\n",
        "\n",
        "                auc_val = np.trapz(pf_score) / (len(pf_score) - 1)\n",
        "                record[f\"{name}_PF_AUC\"] = auc_val\n",
        "\n",
        "                if plot_flag:\n",
        "                    viz = show_cam_on_image(img_np, grayscale_cam, use_rgb=True)\n",
        "                    axes[m_idx+1].imshow(viz)\n",
        "                    axes[m_idx+1].set_title(f\"{name}\\nAUC: {auc_val:.3f}\")\n",
        "                    axes[m_idx+1].axis('off')\n",
        "\n",
        "                # CRITICAL: Manually trigger CAM cleanup if the library supports it\n",
        "                if hasattr(cam_obj, 'activations_and_grads'):\n",
        "                    cam_obj.activations_and_grads.release()\n",
        "\n",
        "            if plot_flag:\n",
        "                plt.savefig(f\"/content/drive/MyDrive/clxai/results_faith/test/pf/{model_version}_sample_{global_idx}.png\")\n",
        "                plt.close(fig)\n",
        "                plt.clf() # Clear the entire current figure\n",
        "\n",
        "            all_scores.append(record)\n",
        "            global_idx += 1\n",
        "\n",
        "            # Clear CAM objects from memory immediately\n",
        "            del curr_methods\n",
        "\n",
        "        # End of batch cleanup\n",
        "        del images, labels, preds\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return pd.DataFrame(all_scores)"
      ],
      "metadata": {
        "id": "_OkJ2xj2UpQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_scores_pf = xai_scores_pf(model=unfrozen_model,\n",
        "#            test_loader=test_loader,\n",
        "#            device=device,\n",
        "#            plot_flag = True) #4 min"
      ],
      "metadata": {
        "id": "Lj7Zp9FAHhJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_scores_pf.to_csv(f\"/content/drive/MyDrive/clxai/results_faith/test_2/{model_version}_pf_xai_scores.csv\", index=False)"
      ],
      "metadata": {
        "id": "EjxvBvaOJwGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IROF"
      ],
      "metadata": {
        "id": "elYjIz5WmXp4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### class - non aprire\n"
      ],
      "metadata": {
        "id": "JdJnPgawoEmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from typing import Any, Callable, Dict, List, Optional\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from quantus.functions.perturb_func import baseline_replacement_by_indices\n",
        "from quantus.helpers import asserts, utils, warn\n",
        "from quantus.helpers.enums import (\n",
        "    DataType,\n",
        "    EvaluationCategory,\n",
        "    ModelType,\n",
        "    ScoreDirection,\n",
        ")\n",
        "from quantus.helpers.model.model_interface import ModelInterface\n",
        "from quantus.helpers.perturbation_utils import make_perturb_func\n",
        "from quantus.metrics.base import Metric\n",
        "\n",
        "if sys.version_info >= (3, 8):\n",
        "    from typing import final\n",
        "else:\n",
        "    from typing_extensions import final\n",
        "\n",
        "\n",
        "@final\n",
        "class IROF(Metric[List[float]]):\n",
        "    \"\"\"\n",
        "    Implementation of IROF (Iterative Removal of Features) by Rieger at el., 2020.\n",
        "\n",
        "    The metric computes the area over the curve per class for sorted mean importances\n",
        "    of feature segments (superpixels) as they are iteratively removed (and prediction scores are collected),\n",
        "    averaged over several test samples.\n",
        "\n",
        "    Assumptions:\n",
        "        - The original metric definition relies on image-segmentation functionality. Therefore, only apply the\n",
        "        metric to 3-dimensional (image) data. To extend the applicablity to other data domains,\n",
        "        adjustments to the current implementation might be necessary.\n",
        "\n",
        "    References:\n",
        "        1) Laura Rieger and Lars Kai Hansen. \"Irof: a low resource evaluation metric for\n",
        "        explanation methods.\" arXiv preprint arXiv:2003.08747 (2020).\n",
        "\n",
        "    Attributes:\n",
        "        -  _name: The name of the metric.\n",
        "        - _data_applicability: The data types that the metric implementation currently supports.\n",
        "        - _models: The model types that this metric can work with.\n",
        "        - score_direction: How to interpret the scores, whether higher/ lower values are considered better.\n",
        "        - evaluation_category: What property/ explanation quality that this metric measures.\n",
        "    \"\"\"\n",
        "\n",
        "    name = \"IROF\"\n",
        "    data_applicability = {DataType.IMAGE}\n",
        "    model_applicability = {ModelType.TORCH, ModelType.TF}\n",
        "    score_direction = ScoreDirection.HIGHER\n",
        "    evaluation_category = EvaluationCategory.FAITHFULNESS\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        segmentation_method: str = \"slic\",\n",
        "        abs: bool = False,\n",
        "        normalise: bool = True,\n",
        "        normalise_func: Optional[Callable[[np.ndarray], np.ndarray]] = None,\n",
        "        normalise_func_kwargs: Optional[Dict[str, Any]] = None,\n",
        "        perturb_func: Optional[Callable] = None,\n",
        "        perturb_baseline: str = \"mean\",\n",
        "        perturb_func_kwargs: Optional[Dict[str, Any]] = None,\n",
        "        return_aggregate: bool = True,\n",
        "        aggregate_func: Optional[Callable] = None,\n",
        "        default_plot_func: Optional[Callable] = None,\n",
        "        disable_warnings: bool = False,\n",
        "        display_progressbar: bool = False,\n",
        "        return_scores: bool = False, # new argument to visualized the curves\n",
        "        distance_based: bool = False, # new argument to determine if evalauation is distance based\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        segmentation_method: string\n",
        "            Image segmentation method:'slic' or 'felzenszwalb', default=\"slic\".\n",
        "        abs: boolean\n",
        "            Indicates whether absolute operation is applied on the attribution, default=False.\n",
        "        normalise: boolean\n",
        "            Indicates whether normalise operation is applied on the attribution, default=True.\n",
        "        normalise_func: callable\n",
        "            Attribution normalisation function applied in case normalise=True.\n",
        "            If normalise_func=None, the default value is used, default=normalise_by_max.\n",
        "        normalise_func_kwargs: dict\n",
        "            Keyword arguments to be passed to normalise_func on call, default={}.\n",
        "        perturb_func: callable\n",
        "            Input perturbation function. If None, the default value is used,\n",
        "            default=baseline_replacement_by_indices.\n",
        "        perturb_baseline: string\n",
        "            Indicates the type of baseline: \"mean\", \"random\", \"uniform\", \"black\" or \"white\",\n",
        "            default=\"mean\".\n",
        "        perturb_func_kwargs: dict\n",
        "            Keyword arguments to be passed to perturb_func, default={}.\n",
        "        return_aggregate: boolean\n",
        "            Indicates if an aggregated score should be computed over all instances.\n",
        "        aggregate_func: callable\n",
        "            Callable that aggregates the scores given an evaluation call.\n",
        "        default_plot_func: callable\n",
        "            Callable that plots the metrics result.\n",
        "        disable_warnings: boolean\n",
        "            Indicates whether the warnings are printed, default=False.\n",
        "        display_progressbar: boolean\n",
        "            Indicates whether a tqdm-progress-bar is printed, default=False.\n",
        "        kwargs: optional\n",
        "            Keyword arguments.\n",
        "        \"\"\"\n",
        "        super().__init__(\n",
        "            abs=abs,\n",
        "            normalise=normalise,\n",
        "            normalise_func=normalise_func,\n",
        "            normalise_func_kwargs=normalise_func_kwargs,\n",
        "            return_aggregate=return_aggregate,\n",
        "            aggregate_func=aggregate_func,\n",
        "            default_plot_func=default_plot_func,\n",
        "            display_progressbar=display_progressbar,\n",
        "            disable_warnings=disable_warnings,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "        if perturb_func is None:\n",
        "            perturb_func = baseline_replacement_by_indices\n",
        "\n",
        "        # Save metric-specific attributes.\n",
        "        self.return_scores = return_scores\n",
        "        self.distance_based = distance_based\n",
        "        self.segmentation_method = segmentation_method\n",
        "        self.nr_channels = None\n",
        "        self.perturb_func = make_perturb_func(\n",
        "            perturb_func, perturb_func_kwargs, perturb_baseline=perturb_baseline\n",
        "        )\n",
        "\n",
        "        # Asserts and warnings.\n",
        "        if not self.disable_warnings:\n",
        "            warn.warn_parameterisation(\n",
        "                metric_name=self.__class__.__name__,\n",
        "                sensitive_params=(\n",
        "                    \"baseline value 'perturb_baseline' and the method to segment \"\n",
        "                    \"the image 'segmentation_method' (including all its associated\"\n",
        "                    \" hyperparameters), also, IROF only works with image data\"\n",
        "                ),\n",
        "                data_domain_applicability=(\n",
        "                    f\"Also, the current implementation only works for 3-dimensional (image) data.\"\n",
        "                ),\n",
        "                citation=(\n",
        "                    \"Rieger, Laura, and Lars Kai Hansen. 'Irof: a low resource evaluation metric \"\n",
        "                    \"for explanation methods.' arXiv preprint arXiv:2003.08747 (2020)\"\n",
        "                ),\n",
        "            )\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        model,\n",
        "        x_batch: np.array,\n",
        "        y_batch: np.array,\n",
        "        a_batch: Optional[np.ndarray] = None,\n",
        "        s_batch: Optional[np.ndarray] = None,\n",
        "        channel_first: Optional[bool] = None,\n",
        "        explain_func: Optional[Callable] = None,\n",
        "        explain_func_kwargs: Optional[Dict] = None,\n",
        "        model_predict_kwargs: Optional[Dict] = None,\n",
        "        softmax: Optional[bool] = True,\n",
        "        device: Optional[str] = None,\n",
        "        batch_size: int = 64,\n",
        "        **kwargs,\n",
        "    ) -> List[float]:\n",
        "        \"\"\"\n",
        "        This implementation represents the main logic of the metric and makes the class object callable.\n",
        "        It completes instance-wise evaluation of explanations (a_batch) with respect to input data (x_batch),\n",
        "        output labels (y_batch) and a torch or tensorflow model (model).\n",
        "\n",
        "        Calls general_preprocess() with all relevant arguments, calls\n",
        "        () on each instance, and saves results to evaluation_scores.\n",
        "        Calls custom_postprocess() afterwards. Finally returns evaluation_scores.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        model: torch.nn.Module, tf.keras.Model\n",
        "            A torch or tensorflow model that is subject to explanation.\n",
        "        x_batch: np.ndarray\n",
        "            A np.ndarray which contains the input data that are explained.\n",
        "        y_batch: np.ndarray\n",
        "            A np.ndarray which contains the output labels that are explained.\n",
        "        a_batch: np.ndarray, optional\n",
        "            A np.ndarray which contains pre-computed attributions i.e., explanations.\n",
        "        s_batch: np.ndarray, optional\n",
        "            A np.ndarray which contains segmentation masks that matches the input.\n",
        "        channel_first: boolean, optional\n",
        "            Indicates of the image dimensions are channel first, or channel last.\n",
        "            Inferred from the input shape if None.\n",
        "        explain_func: callable\n",
        "            Callable generating attributions.\n",
        "        explain_func_kwargs: dict, optional\n",
        "            Keyword arguments to be passed to explain_func on call.\n",
        "        model_predict_kwargs: dict, optional\n",
        "            Keyword arguments to be passed to the model's predict method.\n",
        "        softmax: boolean\n",
        "            Indicates whether to use softmax probabilities or logits in model prediction.\n",
        "            This is used for this __call__ only and won't be saved as attribute. If None, self.softmax is used.\n",
        "        device: string\n",
        "            Indicated the device on which a torch.Tensor is or will be allocated: \"cpu\" or \"gpu\".\n",
        "        kwargs: optional\n",
        "            Keyword arguments.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        evaluation_scores: list\n",
        "            a list of Any with the evaluation scores of the concerned batch.\n",
        "\n",
        "        Examples:\n",
        "        --------\n",
        "            # Minimal imports.\n",
        "            >> import quantus\n",
        "            >> from quantus import LeNet\n",
        "            >> import torch\n",
        "\n",
        "            # Enable GPU.\n",
        "            >> device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "            # Load a pre-trained LeNet classification model (architecture at quantus/helpers/models).\n",
        "            >> model = LeNet()\n",
        "            >> model.load_state_dict(torch.load(\"tutorials/assets/pytests/mnist_model\"))\n",
        "\n",
        "            # Load MNIST datasets and make loaders.\n",
        "            >> test_set = torchvision.datasets.MNIST(root='./sample_data', download=True)\n",
        "            >> test_loader = torch.utils.data.DataLoader(test_set, batch_size=24)\n",
        "\n",
        "            # Load a batch of inputs and outputs to use for XAI evaluation.\n",
        "            >> x_batch, y_batch = iter(test_loader).next()\n",
        "            >> x_batch, y_batch = x_batch.cpu().numpy(), y_batch.cpu().numpy()\n",
        "\n",
        "            # Generate Saliency attributions of the test set batch of the test set.\n",
        "            >> a_batch_saliency = Saliency(model).attribute(inputs=x_batch, target=y_batch, abs=True).sum(axis=1)\n",
        "            >> a_batch_saliency = a_batch_saliency.cpu().numpy()\n",
        "\n",
        "            # Initialise the metric and evaluate explanations by calling the metric instance.\n",
        "            >> metric = Metric(abs=True, normalise=False)\n",
        "            >> scores = metric(model=model, x_batch=x_batch, y_batch=y_batch, a_batch=a_batch_saliency)\n",
        "        \"\"\"\n",
        "        return super().__call__(\n",
        "            model=model,\n",
        "            x_batch=x_batch,\n",
        "            y_batch=y_batch,\n",
        "            a_batch=a_batch,\n",
        "            s_batch=s_batch,\n",
        "            custom_batch=None,\n",
        "            channel_first=channel_first,\n",
        "            explain_func=explain_func,\n",
        "            explain_func_kwargs=explain_func_kwargs,\n",
        "            softmax=softmax,\n",
        "            device=device,\n",
        "            model_predict_kwargs=model_predict_kwargs,\n",
        "            batch_size=batch_size,\n",
        "            **kwargs,\n",
        "        )\n",
        "    def distance_to_centroid(self,\n",
        "            model: ModelInterface,\n",
        "            all_centroids: np.ndarray,\n",
        "            y_pred: np.ndarray,\n",
        "            x_input: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Calculate the Euclidean distance between the predicted embedding of an input and the centroid of the predicted class.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        model : ModelInterface\n",
        "            The model containing the embedding network and classification head.\n",
        "        all_centroids : np.ndarray\n",
        "            An array containing the centroids of all classes.\n",
        "        y_pred : np.ndarray or torch.Tensor\n",
        "            The predicted output from the model, typically a one-hot encoded vector.\n",
        "        x_input : np.ndarray\n",
        "            The input data for which the distance to the class centroid is calculated.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        np.ndarray\n",
        "            The calculated distance between the input's embedding and the predicted class centroid.\n",
        "        \"\"\"\n",
        "\n",
        "        x_input = torch.tensor(x_input, dtype=torch.float32).to(model.device)\n",
        "        y_pred_embedding = model.model.embedding\n",
        "        y_pred_embedding = y_pred_embedding.view(y_pred_embedding.size(0), -1)\n",
        "\n",
        "        if isinstance(y_pred_embedding, torch.Tensor):\n",
        "            y_pred_embedding = y_pred_embedding.detach().cpu().numpy()\n",
        "\n",
        "        if isinstance(y_pred, torch.Tensor):\n",
        "            y_pred = y_pred.detach().cpu().numpy()\n",
        "\n",
        "        y_pred = np.argmax(y_pred)\n",
        "        pred_centroid = all_centroids[y_pred]  # [B, D]\n",
        "        distance = np.linalg.norm(y_pred_embedding - pred_centroid, axis=1)\n",
        "        return distance\n",
        "\n",
        "    def compute_centroids(self,\n",
        "            model: ModelInterface) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Compute the centroids of the clusters in the KNN model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        model : ModelInterface\n",
        "            The model containing the KNN model.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        np.ndarray\n",
        "            An array containing the centroids of all classes.\n",
        "        \"\"\"\n",
        "        knn = model.model.classification_head.knn\n",
        "        centroid = []\n",
        "        num_classes = len(knn.classes_)\n",
        "        for i in range (num_classes):\n",
        "            class_i = knn._fit_X[knn._y == i]\n",
        "\n",
        "            # Compute the centroid\n",
        "            centroid_i = np.mean(class_i, axis = 0)\n",
        "            centroid.append(centroid_i)\n",
        "        return centroid\n",
        "    def evaluate_instance(\n",
        "        self,\n",
        "        model: ModelInterface,\n",
        "        x: np.ndarray,\n",
        "        y: np.ndarray,\n",
        "        a: np.ndarray,\n",
        "    ) -> float:\n",
        "        \"\"\"\n",
        "        Evaluate instance gets model and data for a single instance as input and returns the evaluation result.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        model: ModelInterface\n",
        "            A ModelInteface that is subject to explanation.\n",
        "        x: np.ndarray\n",
        "            The input to be evaluated on an instance-basis.\n",
        "        y: np.ndarray\n",
        "            The output to be evaluated on an instance-basis.\n",
        "        a: np.ndarray\n",
        "            The explanation to be evaluated on an instance-basis.\n",
        "        Returns\n",
        "        -------\n",
        "        float\n",
        "            The evaluation results.\n",
        "        \"\"\"\n",
        "        # Predict on x.\n",
        "        x_input = model.shape_input(x, x.shape, channel_first=True)\n",
        "\n",
        "        ############ outputs a class ################\n",
        "        # need to find the distanceto class centroid and store\n",
        "        # same \"similar to probability\" -> moght be more interesting if the classification was k+1 because a fully erased thing makes no sense to be classified as a cat or a dog --> but how do i train the model?\n",
        "        if self.distance_based:\n",
        "            y_pred = model.predict(x_input)\n",
        "            centroid_array = self.compute_centroids(model = model)\n",
        "            distance = self.distance_to_centroid(model = model, all_centroids = centroid_array, y_pred = y_pred, x_input = x_input)\n",
        "            y_pred = distance\n",
        "\n",
        "        # Calculate the area over the curve (AOC) score.\n",
        "        # higher is worse so the sign of preds needs to be changed\n",
        "        # higher is better so the sign of preds does not need to be changed\n",
        "        #############################################\n",
        "        else:\n",
        "            y_pred = float(model.predict(x_input)[:, y])\n",
        "\n",
        "        # Segment image.\n",
        "        segments = utils.get_superpixel_segments(\n",
        "            img=np.moveaxis(x, 0, -1).astype(\"double\"),\n",
        "            segmentation_method=self.segmentation_method,\n",
        "        )\n",
        "        nr_segments = len(np.unique(segments))\n",
        "        asserts.assert_nr_segments(nr_segments=nr_segments)\n",
        "\n",
        "        # Calculate average attribution of each segment.\n",
        "        att_segs = np.zeros(nr_segments)\n",
        "        for i, s in enumerate(range(nr_segments)):\n",
        "            att_segs[i] = np.mean(a[:, segments == s])\n",
        "\n",
        "        # Sort segments based on the mean attribution (descending order).\n",
        "        s_indices = np.argsort(-att_segs)\n",
        "\n",
        "        preds = []\n",
        "        x_prev_perturbed = x\n",
        "\n",
        "        for i_ix, s_ix in enumerate(s_indices):\n",
        "            # Perturb input by indices of attributions.\n",
        "            a_ix = np.nonzero((segments == s_ix).flatten())[0]\n",
        "\n",
        "            x_perturbed = self.perturb_func(\n",
        "                arr=x_prev_perturbed,\n",
        "                indices=a_ix,\n",
        "                indexed_axes=self.a_axes,\n",
        "            )\n",
        "            warn.warn_perturbation_caused_no_change(\n",
        "                x=x_prev_perturbed, x_perturbed=x_perturbed\n",
        "            )\n",
        "\n",
        "            # Predict on perturbed input x.\n",
        "            x_input = model.shape_input(x_perturbed, x.shape, channel_first=True)\n",
        "\n",
        "\n",
        "            ### changes #####\n",
        "            if self.distance_based:\n",
        "                y_pred_perturb = model.predict(x_input)\n",
        "                distance_perturb = self.distance_to_centroid(model = model, all_centroids = centroid_array, y_pred = y_pred_perturb, x_input =x_input)\n",
        "                y_pred_perturb = distance_perturb\n",
        "            else:\n",
        "                y_pred_perturb = float(model.predict(x_input)[:, y])\n",
        "\n",
        "\n",
        "            # Normalise the scores to be within range [0, 1].\n",
        "            preds.append(float(y_pred_perturb / y_pred))\n",
        "            x_prev_perturbed = x_perturbed\n",
        "\n",
        "        if self.distance_based:\n",
        "            # inverting so the greater the distance the smaller the number and auc works correctly\n",
        "            preds = 1.0 / (np.array(preds) + 1e-8)\n",
        "\n",
        "        # Calculate the area over the curve (AOC) score.\n",
        "        aoc = len(preds) - utils.calculate_auc(np.array(preds))\n",
        "        if self.return_scores:\n",
        "            return aoc, preds\n",
        "        return aoc\n",
        "\n",
        "    def custom_preprocess(\n",
        "        self,\n",
        "        x_batch: np.ndarray,\n",
        "        **kwargs,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Implementation of custom_preprocess_batch.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        model: torch.nn.Module, tf.keras.Model\n",
        "            A torch or tensorflow model e.g., torchvision.models that is subject to explanation.\n",
        "        x_batch: np.ndarray\n",
        "            A np.ndarray which contains the input data that are explained.\n",
        "        y_batch: np.ndarray\n",
        "            A np.ndarray which contains the output labels that are explained.\n",
        "        a_batch: np.ndarray, optional\n",
        "            A np.ndarray which contains pre-computed attributions i.e., explanations.\n",
        "        s_batch: np.ndarray, optional\n",
        "            A np.ndarray which contains segmentation masks that matches the input.\n",
        "        custom_batch: any\n",
        "            Gives flexibility ot the user to use for evaluation, can hold any variable.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        None\n",
        "        \"\"\"\n",
        "        # Infer number of input channels.\n",
        "        self.nr_channels = x_batch.shape[1]\n",
        "\n",
        "    @property\n",
        "    def get_aoc_score(self):\n",
        "        \"\"\"Calculate the area over the curve (AOC) score for several test samples.\"\"\"\n",
        "        return np.mean(self.evaluation_scores)\n",
        "\n",
        "    def evaluate_batch(\n",
        "        self,\n",
        "        model: ModelInterface,\n",
        "        x_batch: np.ndarray,\n",
        "        y_batch: np.ndarray,\n",
        "        a_batch: np.ndarray,\n",
        "        **kwargs,\n",
        "    ) -> List[float]:\n",
        "        \"\"\"\n",
        "        This method performs XAI evaluation on a single batch of explanations.\n",
        "        For more information on the specific logic, we refer the metrics initialisation docstring.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        model: ModelInterface\n",
        "            A ModelInterface that is subject to explanation.\n",
        "        x_batch: np.ndarray\n",
        "            The input to be evaluated on a batch-basis.\n",
        "        y_batch: np.ndarray\n",
        "            The output to be evaluated on a batch-basis.\n",
        "        a_batch: np.ndarray\n",
        "            The explanation to be evaluated on a batch-basis.\n",
        "        kwargs:\n",
        "            Unused.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        scores_batch:\n",
        "            The evaluation results.\n",
        "        \"\"\"\n",
        "        return [\n",
        "            self.evaluate_instance(model=model, x=x, y=y, a=a)\n",
        "            for x, y, a in zip(x_batch, y_batch, a_batch)\n",
        "        ]\n",
        "\n",
        "    ############# new ##########\n",
        "    def custom_postprocess(self, **kwargs):\n",
        "        if self.return_scores:\n",
        "            if self.default_plot_func is not None:\n",
        "                return self.default_plot_func(self.evaluation_scores)\n",
        "            return self.evaluation_scores"
      ],
      "metadata": {
        "id": "3KA66yZqYZst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### app"
      ],
      "metadata": {
        "id": "nhpgTi_UoH5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def xai_scores_irof(model, test_loader, device, plot_flag=False,\n",
        "                    segmentation_method=\"slic\", perturb_baseline=\"black\", distance_based=False):\n",
        "\n",
        "    model.eval()\n",
        "    target_layers = [model.encoder.layer4[-1]]\n",
        "\n",
        "    irof_metric = IROF(\n",
        "        segmentation_method=segmentation_method,\n",
        "        perturb_baseline=perturb_baseline,\n",
        "        abs=False,\n",
        "        normalise=True,\n",
        "        disable_warnings=True,\n",
        "        return_scores=True,  # Per ottenere sia AOC che la curva\n",
        "        distance_based=distance_based,\n",
        "        return_aggregate=False,  # IMPORTANTE: disabilita l'aggregazione automatica\n",
        "    )\n",
        "\n",
        "    all_scores = []\n",
        "    global_idx = 0\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(test_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            preds = model(images).argmax(dim=1)\n",
        "\n",
        "        for i in range(images.size(0)):\n",
        "            p_idx = preds[i].item()\n",
        "            g_idx = labels[i].item()\n",
        "            img_tensor = images[i:i+1]\n",
        "\n",
        "            record = {\"idx\": global_idx, \"true\": g_idx, \"pred\": p_idx}\n",
        "\n",
        "            # Inizializza i metodi CAM\n",
        "            curr_methods = {\n",
        "                \"GradCAM\": GradCAM(model=model, target_layers=target_layers),\n",
        "                \"EigenCAM\": EigenCAM(model=model, target_layers=target_layers),\n",
        "                \"AblationCAM\": AblationCAM(model=model, target_layers=target_layers)\n",
        "            }\n",
        "\n",
        "            if plot_flag:\n",
        "                fig, axes = plt.subplots(1, 4, figsize=(18, 5))\n",
        "                img_np = images[i].permute(1, 2, 0).cpu().numpy()\n",
        "                img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min() + 1e-8)\n",
        "                axes[0].imshow(img_np)\n",
        "                axes[0].axis('off')\n",
        "                axes[0].set_title(\"Original Image\")\n",
        "\n",
        "            for m_idx, (name, cam_obj) in enumerate(curr_methods.items()):\n",
        "                # Genera la mappa di saliency\n",
        "                with torch.enable_grad():\n",
        "                    grayscale_cam = cam_obj(\n",
        "                        input_tensor=img_tensor,\n",
        "                        targets=[ClassifierOutputTarget(p_idx)]\n",
        "                    )[0, :]  # Shape: [H, W]\n",
        "\n",
        "                # Prepara l'input per IROF\n",
        "                x_np = img_tensor.detach().cpu().numpy()  # [1, C, H, W]\n",
        "\n",
        "                # Espandi grayscale_cam per avere la dimensione dei canali\n",
        "                a_np = np.repeat(grayscale_cam[np.newaxis, :, :], x_np.shape[1], axis=0)  # [C, H, W]\n",
        "                a_np = a_np[np.newaxis, ...]  # [1, C, H, W]\n",
        "\n",
        "                # Calcola IROF\n",
        "                try:\n",
        "                    irof_result = irof_metric(\n",
        "                        model=model,\n",
        "                        x_batch=x_np,\n",
        "                        y_batch=np.array([g_idx]),\n",
        "                        a_batch=a_np,\n",
        "                        device=device\n",
        "                    )\n",
        "\n",
        "                    # Con return_scores=True e return_aggregate=False,\n",
        "                    # irof_result  una lista: [(aoc, preds)]\n",
        "                    if isinstance(irof_result, list) and len(irof_result) > 0:\n",
        "                        result_item = irof_result[0]\n",
        "\n",
        "                        if isinstance(result_item, tuple) and len(result_item) == 2:\n",
        "                            # Estrai AOC e curva\n",
        "                            aoc_score, irof_curve = result_item\n",
        "                            record[f\"{name}_IROF_AOC\"] = float(aoc_score)\n",
        "\n",
        "                            # Salva la curva come lista\n",
        "                            if isinstance(irof_curve, np.ndarray):\n",
        "                                record[f\"{name}_IROF_curve\"] = irof_curve.tolist()\n",
        "                            else:\n",
        "                                record[f\"{name}_IROF_curve\"] = list(irof_curve)\n",
        "                        else:\n",
        "                            # Fallback: solo AOC\n",
        "                            aoc_score = float(result_item)\n",
        "                            record[f\"{name}_IROF_AOC\"] = aoc_score\n",
        "                    else:\n",
        "                        print(f\"Formato risultato inaspettato per {name}, sample {global_idx}\")\n",
        "                        record[f\"{name}_IROF_AOC\"] = np.nan\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Errore IROF per {name}, sample {global_idx}: {str(e)}\")\n",
        "                    import traceback\n",
        "                    traceback.print_exc()\n",
        "                    record[f\"{name}_IROF_AOC\"] = np.nan\n",
        "                    continue\n",
        "\n",
        "                if plot_flag:\n",
        "                    viz = show_cam_on_image(img_np, grayscale_cam, use_rgb=True)\n",
        "                    axes[m_idx+1].imshow(viz)\n",
        "\n",
        "                    # Mostra AOC nel titolo\n",
        "                    aoc_val = record.get(f\"{name}_IROF_AOC\", np.nan)\n",
        "                    if not np.isnan(aoc_val):\n",
        "                        axes[m_idx+1].set_title(f\"{name}\\nIROF AOC: {aoc_val:.3f}\")\n",
        "                    else:\n",
        "                        axes[m_idx+1].set_title(f\"{name}\\nIROF: Error\")\n",
        "                    axes[m_idx+1].axis('off')\n",
        "\n",
        "                # Cleanup CAM\n",
        "                if hasattr(cam_obj, 'activations_and_grads'):\n",
        "                    cam_obj.activations_and_grads.release()\n",
        "\n",
        "            if plot_flag:\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(f\"/content/drive/MyDrive/clxai/results_faith/test/irof/{model_version}_sample_{global_idx}.png\", bbox_inches='tight', dpi=150)\n",
        "                plt.close(fig)\n",
        "                plt.clf()\n",
        "\n",
        "            all_scores.append(record)\n",
        "            global_idx += 1\n",
        "\n",
        "            # Cleanup\n",
        "            del curr_methods\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        # Cleanup batch\n",
        "        del images, labels, preds\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return pd.DataFrame(all_scores)"
      ],
      "metadata": {
        "id": "hJ21abcUaAlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# irof_scores = xai_scores_irof(\n",
        "#     model = unfrozen_model,\n",
        "#     test_loader = test_loader,\n",
        "#     device = device,\n",
        "#     plot_flag = True,\n",
        "# )"
      ],
      "metadata": {
        "id": "a5VPk6dJaMdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# irof_scores.to_csv(f\"/content/drive/MyDrive/clxai/results_faith/test_2/{model_version}_irof_xai_scores.csv\", index=False)"
      ],
      "metadata": {
        "id": "EUKE3Ecwdqj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sparseness"
      ],
      "metadata": {
        "id": "l0Kp8HQTpBYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def xai_scores_sparsity(model, test_loader, device, plot_flag=False):\n",
        "    model.eval()\n",
        "    target_layers = [model.encoder.layer4[-1]]\n",
        "\n",
        "    sparsity_metric = quantus.Sparseness(\n",
        "        disable_warnings=True,\n",
        "    )\n",
        "\n",
        "    all_scores = []\n",
        "    global_idx = 0\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(test_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            preds = model(images).argmax(dim=1)\n",
        "\n",
        "        for i in range(images.size(0)):\n",
        "            p_idx = preds[i].item()\n",
        "            g_idx = labels[i].item()\n",
        "            img_tensor = images[i:i+1]\n",
        "\n",
        "            record = {\"idx\": global_idx, \"true\": g_idx, \"pred\": p_idx}\n",
        "\n",
        "            curr_methods = {\n",
        "                \"GradCAM\": GradCAM(model=model, target_layers=target_layers),\n",
        "                \"EigenCAM\": EigenCAM(model=model, target_layers=target_layers),\n",
        "                \"AblationCAM\": AblationCAM(model=model, target_layers=target_layers)\n",
        "            }\n",
        "\n",
        "            if plot_flag:\n",
        "                fig, axes = plt.subplots(1, 4, figsize=(18, 5))\n",
        "                img_np = images[i].permute(1, 2, 0).cpu().numpy()\n",
        "                img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min() + 1e-8)\n",
        "                axes[0].imshow(img_np)\n",
        "                axes[0].axis('off')\n",
        "\n",
        "            for m_idx, (name, cam_obj) in enumerate(curr_methods.items()):\n",
        "                with torch.enable_grad():\n",
        "                    grayscale_cam = cam_obj(input_tensor=img_tensor,\n",
        "                                           targets=[ClassifierOutputTarget(p_idx)])[0, :]\n",
        "\n",
        "                # Calcolo della Sparsity\n",
        "                # Restituisce un valore singolo (float) tra 0 e 1 per ogni spiegazione\n",
        "                sparsity_score = sparsity_metric(\n",
        "                    model=model,\n",
        "                    x_batch=img_tensor.detach().cpu().numpy(),\n",
        "                    y_batch=np.array([g_idx]),\n",
        "                    a_batch=grayscale_cam[np.newaxis, ...],\n",
        "                    device=device\n",
        "                )[0]\n",
        "\n",
        "                record[f\"{name}_Sparsity\"] = sparsity_score\n",
        "\n",
        "                if plot_flag:\n",
        "                    viz = show_cam_on_image(img_np, grayscale_cam, use_rgb=True)\n",
        "                    axes[m_idx+1].imshow(viz)\n",
        "                    axes[m_idx+1].set_title(f\"{name}\\nSparse: {sparsity_score:.3f}\")\n",
        "                    axes[m_idx+1].axis('off')\n",
        "\n",
        "                if hasattr(cam_obj, 'activations_and_grads'):\n",
        "                    cam_obj.activations_and_grads.release()\n",
        "\n",
        "            if plot_flag:\n",
        "                plt.savefig(f\"/content/drive/MyDrive/clxai/results_complex/test/sparseness/{model_version}_sample_{global_idx}_sparsity.png\")\n",
        "                plt.close(fig)\n",
        "\n",
        "            all_scores.append(record)\n",
        "            global_idx += 1\n",
        "            del curr_methods\n",
        "\n",
        "        del images, labels, preds\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return pd.DataFrame(all_scores)"
      ],
      "metadata": {
        "id": "W0baKKRbpHCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sparse_scores = xai_scores_sparsity(\n",
        "#     model = unfrozen_model,\n",
        "#     test_loader = test_loader,\n",
        "#     device = device,\n",
        "#     plot_flag = True,\n",
        "# )"
      ],
      "metadata": {
        "id": "an2bF-gKpw90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sparse_scores.to_csv(f\"/content/drive/MyDrive/clxai/results_complex/test2/{model_version}_sparse_xai_scores.csv\", index=False)"
      ],
      "metadata": {
        "id": "odahNY-Dqms2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## complexity"
      ],
      "metadata": {
        "id": "siS_uxP7pFNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def xai_scores_complexity(model, test_loader, device, plot_flag=False):\n",
        "    model.eval()\n",
        "    target_layers = [model.encoder.layer4[-1]]\n",
        "\n",
        "    complexity_metric = quantus.Complexity(\n",
        "        disable_warnings=True,\n",
        "    )\n",
        "\n",
        "    all_scores = []\n",
        "    global_idx = 0\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(test_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            preds = model(images).argmax(dim=1)\n",
        "\n",
        "        for i in range(images.size(0)):\n",
        "            p_idx = preds[i].item()\n",
        "            g_idx = labels[i].item()\n",
        "            img_tensor = images[i:i+1]\n",
        "\n",
        "            record = {\"idx\": global_idx, \"true\": g_idx, \"pred\": p_idx}\n",
        "\n",
        "            curr_methods = {\n",
        "                \"GradCAM\": GradCAM(model=model, target_layers=target_layers),\n",
        "                \"EigenCAM\": EigenCAM(model=model, target_layers=target_layers),\n",
        "                \"AblationCAM\": AblationCAM(model=model, target_layers=target_layers)\n",
        "            }\n",
        "\n",
        "            if plot_flag:\n",
        "                fig, axes = plt.subplots(1, 4, figsize=(18, 5))\n",
        "                img_np = images[i].permute(1, 2, 0).cpu().numpy()\n",
        "                img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min() + 1e-8)\n",
        "                axes[0].imshow(img_np)\n",
        "                axes[0].axis('off')\n",
        "\n",
        "            for m_idx, (name, cam_obj) in enumerate(curr_methods.items()):\n",
        "                with torch.enable_grad():\n",
        "                    grayscale_cam = cam_obj(input_tensor=img_tensor,\n",
        "                                           targets=[ClassifierOutputTarget(p_idx)])[0, :]\n",
        "\n",
        "                complexity_score = complexity_metric(\n",
        "                    model=model,\n",
        "                    x_batch=img_tensor.detach().cpu().numpy(),\n",
        "                    y_batch=np.array([g_idx]),\n",
        "                    a_batch=grayscale_cam[np.newaxis, ...],\n",
        "                    device=device\n",
        "                )[0]\n",
        "\n",
        "                record[f\"{name}_Complexity\"] = complexity_score\n",
        "\n",
        "                if plot_flag:\n",
        "                    viz = show_cam_on_image(img_np, grayscale_cam, use_rgb=True)\n",
        "                    axes[m_idx+1].imshow(viz)\n",
        "                    axes[m_idx+1].set_title(f\"{name}\\nSparse: {complexity_score:.3f}\")\n",
        "                    axes[m_idx+1].axis('off')\n",
        "\n",
        "                if hasattr(cam_obj, 'activations_and_grads'):\n",
        "                    cam_obj.activations_and_grads.release()\n",
        "\n",
        "            if plot_flag:\n",
        "                plt.savefig(f\"/content/drive/MyDrive/clxai/results_complex/test/complexity/{model_version}_sample_{global_idx}_complexity.png\")\n",
        "                plt.close(fig)\n",
        "\n",
        "            all_scores.append(record)\n",
        "            global_idx += 1\n",
        "            del curr_methods\n",
        "\n",
        "        del images, labels, preds\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return pd.DataFrame(all_scores)"
      ],
      "metadata": {
        "id": "hCEhtuDqtwEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# complex_scores = xai_scores_complexity(\n",
        "#     model = unfrozen_model,\n",
        "#     test_loader = test_loader,\n",
        "#     device = device,\n",
        "#     plot_flag = True,\n",
        "# )"
      ],
      "metadata": {
        "id": "i4XmUaA6tyll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# complex_scores.to_csv(f\"/content/drive/MyDrive/clxai/results_complex/test2/{model_version}_complex_xai_scores.csv\", index=False)"
      ],
      "metadata": {
        "id": "vp7Cl7R-t0xI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## contrastivity"
      ],
      "metadata": {
        "id": "Ktt1wxFm40_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "def xai_scores_ssim_robustness(model, test_loader, device, plot_flag=False, noise_level=0.02):\n",
        "    model.eval()\n",
        "    target_layers = [model.encoder.layer4[-1]]\n",
        "    all_scores = []\n",
        "    global_idx = 0\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(test_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            preds = model(images).argmax(dim=1)\n",
        "\n",
        "        for i in range(images.size(0)):\n",
        "\n",
        "            torch.manual_seed(global_idx)\n",
        "            np.random.seed(global_idx)\n",
        "\n",
        "            p_idx = preds[i].item()\n",
        "            g_idx = labels[i].item()\n",
        "            img_tensor = images[i:i+1]\n",
        "\n",
        "            perturbed_tensor = img_tensor + torch.randn_like(img_tensor) * noise_level\n",
        "            perturbed_tensor = torch.clamp(perturbed_tensor, 0, 1)\n",
        "\n",
        "            record = {\"idx\": global_idx, \"true\": g_idx, \"pred\": p_idx}\n",
        "\n",
        "            methods_list = [\n",
        "                (\"GradCAM\", GradCAM(model=model, target_layers=target_layers)),\n",
        "                (\"EigenCAM\", EigenCAM(model=model, target_layers=target_layers)),\n",
        "                (\"AblationCAM\", AblationCAM(model=model, target_layers=target_layers))\n",
        "            ]\n",
        "\n",
        "            if plot_flag:\n",
        "                fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "\n",
        "                img_np = images[i].permute(1, 2, 0).cpu().numpy()\n",
        "                img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min() + 1e-8)\n",
        "\n",
        "                pert_np = perturbed_tensor[0].permute(1, 2, 0).cpu().numpy()\n",
        "                pert_np = (pert_np - pert_np.min()) / (pert_np.max() - pert_np.min() + 1e-8)\n",
        "\n",
        "                axes[0, 0].imshow(img_np)\n",
        "                axes[0, 0].set_title(\"Original Input\")\n",
        "                axes[1, 0].imshow(pert_np)\n",
        "                axes[1, 0].set_title(f\"Perturbed (Noise: {noise_level})\")\n",
        "                for r in range(2): axes[r, 0].axis('off')\n",
        "\n",
        "            for m_idx, (name, cam_obj) in enumerate(methods_list):\n",
        "                with torch.enable_grad():\n",
        "\n",
        "                    cam_orig = cam_obj(input_tensor=img_tensor,\n",
        "                                       targets=[ClassifierOutputTarget(p_idx)])[0, :]\n",
        "\n",
        "                    cam_pert = cam_obj(input_tensor=perturbed_tensor,\n",
        "                                       targets=[ClassifierOutputTarget(p_idx)])[0, :]\n",
        "\n",
        "                robustness_score = ssim(cam_orig, cam_pert, data_range=1.0)\n",
        "                record[f\"{name}_SSIM_Robustness\"] = robustness_score\n",
        "\n",
        "                if plot_flag:\n",
        "                    viz_orig = show_cam_on_image(img_np, cam_orig, use_rgb=True)\n",
        "                    axes[0, m_idx+1].imshow(viz_orig)\n",
        "                    axes[0, m_idx+1].set_title(f\"{name} (Orig)\")\n",
        "\n",
        "                    viz_pert = show_cam_on_image(pert_np, cam_pert, use_rgb=True)\n",
        "                    axes[1, m_idx+1].imshow(viz_pert)\n",
        "                    axes[1, m_idx+1].set_title(f\"{name} (Perturbed)\\nSSIM Robustness: {robustness_score:.3f}\")\n",
        "\n",
        "                    for r in range(2): axes[r, m_idx+1].axis('off')\n",
        "\n",
        "                if hasattr(cam_obj, 'activations_and_grads'):\n",
        "                    cam_obj.activations_and_grads.release()\n",
        "\n",
        "            if plot_flag:\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(f\"/content/drive/MyDrive/clxai/results_contrast/test/robustness/{model_version}_ssim_robustness_sample_{global_idx}.png\")\n",
        "                plt.close(fig)\n",
        "\n",
        "            all_scores.append(record)\n",
        "            global_idx += 1\n",
        "\n",
        "        del images, labels, preds\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return pd.DataFrame(all_scores)"
      ],
      "metadata": {
        "id": "_s8Rz-Dl6zZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# robust_scores = xai_scores_ssim_robustness(\n",
        "#     model = unfrozen_model,\n",
        "#     test_loader = test_loader,\n",
        "#     device = device,\n",
        "#     plot_flag = True,\n",
        "# )"
      ],
      "metadata": {
        "id": "vQ-gSxtL8JYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# robust_scores.to_csv(f\"/content/drive/MyDrive/clxai/results_contrast/test2/{model_version}_robust_xai_scores.csv\", index=False)"
      ],
      "metadata": {
        "id": "3NAS6H5d9lBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## constrastivity 2"
      ],
      "metadata": {
        "id": "xMoGFYiKFD99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "def xai_scores_ssim_contrastivity(model, test_loader, device, plot_flag=False):\n",
        "    model.eval()\n",
        "    target_layers = [model.encoder.layer4[-1]]\n",
        "    all_scores = []\n",
        "    global_idx = 0\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(test_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(images)\n",
        "            preds = logits.argmax(dim=1)\n",
        "\n",
        "            N, C = logits.shape\n",
        "            r = torch.randint(0, C - 1, (N,), device=device)\n",
        "            contrast_preds = r + (r >= preds).long()\n",
        "\n",
        "        for i in range(images.size(0)):\n",
        "            p_idx = preds[i].item()\n",
        "            c_idx = contrast_preds[i].item() # Classe di contrasto\n",
        "            g_idx = labels[i].item()\n",
        "            img_tensor = images[i:i+1]\n",
        "\n",
        "            record = {\n",
        "                \"idx\": global_idx,\n",
        "                \"true\": g_idx,\n",
        "                \"pred\": p_idx,\n",
        "                \"contrast_class\": c_idx\n",
        "            }\n",
        "\n",
        "            methods_list = [\n",
        "                (\"GradCAM\", GradCAM(model=model, target_layers=target_layers)),\n",
        "                (\"EigenCAM\", EigenCAM(model=model, target_layers=target_layers)),\n",
        "                (\"AblationCAM\", AblationCAM(model=model, target_layers=target_layers))\n",
        "            ]\n",
        "\n",
        "            if plot_flag:\n",
        "                fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "                img_np = images[i].permute(1, 2, 0).cpu().numpy()\n",
        "                img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min() + 1e-8)\n",
        "\n",
        "                axes[0, 0].imshow(img_np)\n",
        "                axes[0, 0].set_title(f\"Input (Label: {g_idx})\")\n",
        "                axes[1, 0].axis('off')\n",
        "                axes[0, 0].axis('off')\n",
        "\n",
        "            for m_idx, (name, cam_obj) in enumerate(methods_list):\n",
        "                with torch.enable_grad():\n",
        "                    cam_factual = cam_obj(input_tensor=img_tensor,\n",
        "                                          targets=[ClassifierOutputTarget(p_idx)])[0, :]\n",
        "\n",
        "                    cam_contrastive = cam_obj(input_tensor=img_tensor,\n",
        "                                              targets=[ClassifierOutputTarget(c_idx)])[0, :]\n",
        "\n",
        "                # SSIM Contrastivity: 1 uguali - 0 indipendenti - <0 opposte\n",
        "                contrast_ssim = ssim(cam_factual, cam_contrastive, data_range=1.0)\n",
        "\n",
        "                # L2 Contrastivity\n",
        "                contrast_l2 = np.linalg.norm(cam_factual - cam_contrastive)\n",
        "\n",
        "                record[f\"{name}_SSIM_Contrastivity\"] = contrast_ssim\n",
        "                record[f\"{name}_L2_Contrastivity\"] = contrast_l2\n",
        "\n",
        "                if plot_flag:\n",
        "                    viz_f = show_cam_on_image(img_np, cam_factual, use_rgb=True)\n",
        "                    axes[0, m_idx+1].imshow(viz_f)\n",
        "                    axes[0, m_idx+1].set_title(f\"{name}\\nPred: {p_idx}\")\n",
        "\n",
        "                    viz_c = show_cam_on_image(img_np, cam_contrastive, use_rgb=True)\n",
        "                    axes[1, m_idx+1].imshow(viz_c)\n",
        "                    axes[1, m_idx+1].set_title(f\"\\nContrast: {c_idx}\\nSSIM Diff: {contrast_ssim:.3f}\\nL2 Norm: {contrast_l2:.3f}\")\n",
        "\n",
        "                    for r in range(2): axes[r, m_idx+1].axis('off')\n",
        "\n",
        "                if hasattr(cam_obj, 'activations_and_grads'):\n",
        "                    cam_obj.activations_and_grads.release()\n",
        "\n",
        "            if plot_flag:\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(f\"/content/drive/MyDrive/clxai/results_contrast/test/contrastivity/{model_version}_contrastivity_{global_idx}.png\")\n",
        "                plt.close(fig)\n",
        "\n",
        "            all_scores.append(record)\n",
        "            global_idx += 1\n",
        "\n",
        "        del images, labels, preds, contrast_preds\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return pd.DataFrame(all_scores)"
      ],
      "metadata": {
        "id": "dYTWBvW7E5tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contrastivity_scores = xai_scores_ssim_contrastivity(\n",
        "    model = unfrozen_model,\n",
        "    test_loader = test_loader,\n",
        "    device = device,\n",
        "    plot_flag = True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inxe0MOFFK8R",
        "outputId": "e21c1b0a-35e1-4350-819f-add8be1a3543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 16/16 [00:00<00:00, 79.91it/s]\n",
            "100%|| 16/16 [00:00<00:00, 95.25it/s]\n",
            "100%|| 16/16 [00:00<00:00, 98.29it/s]\n",
            "100%|| 16/16 [00:00<00:00, 102.75it/s]\n",
            "100%|| 16/16 [00:00<00:00, 84.49it/s]\n",
            "100%|| 16/16 [00:00<00:00, 83.05it/s]\n",
            "100%|| 16/16 [00:00<00:00, 80.69it/s]\n",
            "100%|| 16/16 [00:00<00:00, 85.57it/s]\n",
            "100%|| 16/16 [00:00<00:00, 61.03it/s]\n",
            "100%|| 16/16 [00:00<00:00, 88.67it/s]\n",
            "100%|| 16/16 [00:00<00:00, 50.26it/s]\n",
            "100%|| 16/16 [00:00<00:00, 39.76it/s]\n",
            "100%|| 16/16 [00:00<00:00, 49.71it/s]\n",
            "100%|| 16/16 [00:00<00:00, 52.00it/s]\n",
            "100%|| 16/16 [00:00<00:00, 60.40it/s]\n",
            "100%|| 16/16 [00:00<00:00, 70.76it/s]\n",
            "100%|| 16/16 [00:00<00:00, 54.92it/s]\n",
            "100%|| 16/16 [00:00<00:00, 70.50it/s]\n",
            "100%|| 16/16 [00:00<00:00, 60.00it/s]\n",
            "100%|| 16/16 [00:00<00:00, 80.73it/s]\n",
            "100%|| 16/16 [00:00<00:00, 59.77it/s]\n",
            "100%|| 16/16 [00:00<00:00, 58.30it/s]\n",
            "100%|| 16/16 [00:00<00:00, 64.99it/s]\n",
            "100%|| 16/16 [00:00<00:00, 87.25it/s]\n",
            "100%|| 16/16 [00:00<00:00, 97.67it/s]\n",
            "100%|| 16/16 [00:00<00:00, 97.81it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.23it/s]\n",
            "100%|| 16/16 [00:00<00:00, 101.14it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.94it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.97it/s]\n",
            "100%|| 16/16 [00:00<00:00, 99.47it/s]\n",
            "100%|| 16/16 [00:00<00:00, 97.58it/s]\n",
            "100%|| 16/16 [00:00<00:00, 92.58it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.72it/s]\n",
            "100%|| 16/16 [00:00<00:00, 99.35it/s]\n",
            "100%|| 16/16 [00:00<00:00, 96.41it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.73it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.85it/s]\n",
            "100%|| 16/16 [00:00<00:00, 99.36it/s]\n",
            "100%|| 16/16 [00:00<00:00, 101.06it/s]\n",
            "100%|| 16/16 [00:00<00:00, 98.86it/s]\n",
            "100%|| 16/16 [00:00<00:00, 101.60it/s]\n",
            "100%|| 16/16 [00:00<00:00, 95.41it/s]\n",
            "100%|| 16/16 [00:00<00:00, 101.60it/s]\n",
            "100%|| 16/16 [00:00<00:00, 99.60it/s]\n",
            "100%|| 16/16 [00:00<00:00, 101.22it/s]\n",
            "100%|| 16/16 [00:00<00:00, 79.36it/s]\n",
            "100%|| 16/16 [00:00<00:00, 89.99it/s]\n",
            "100%|| 16/16 [00:00<00:00, 74.85it/s]\n",
            "100%|| 16/16 [00:00<00:00, 85.47it/s]\n",
            "100%|| 16/16 [00:00<00:00, 81.07it/s]\n",
            "100%|| 16/16 [00:00<00:00, 85.48it/s]\n",
            "100%|| 16/16 [00:00<00:00, 97.89it/s]\n",
            "100%|| 16/16 [00:00<00:00, 99.92it/s] \n",
            "100%|| 16/16 [00:00<00:00, 95.63it/s]\n",
            "100%|| 16/16 [00:00<00:00, 102.01it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.62it/s]\n",
            "100%|| 16/16 [00:00<00:00, 96.71it/s]\n",
            "100%|| 16/16 [00:00<00:00, 99.90it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.12it/s]\n",
            "100%|| 16/16 [00:00<00:00, 99.32it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.16it/s]\n",
            "100%|| 16/16 [00:00<00:00, 99.00it/s]\n",
            "100%|| 16/16 [00:00<00:00, 101.67it/s]\n",
            "100%|| 16/16 [00:00<00:00, 89.93it/s]\n",
            "100%|| 16/16 [00:00<00:00, 98.76it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.02it/s]\n",
            "100%|| 16/16 [00:00<00:00, 97.11it/s]\n",
            "100%|| 16/16 [00:00<00:00, 94.97it/s]\n",
            "100%|| 16/16 [00:00<00:00, 101.20it/s]\n",
            "100%|| 16/16 [00:00<00:00, 98.93it/s]\n",
            "100%|| 16/16 [00:00<00:00, 97.28it/s]\n",
            "100%|| 16/16 [00:00<00:00, 96.93it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.73it/s]\n",
            "100%|| 16/16 [00:00<00:00, 90.88it/s]\n",
            "100%|| 16/16 [00:00<00:00, 101.76it/s]\n",
            "100%|| 16/16 [00:00<00:00, 87.22it/s]\n",
            "100%|| 16/16 [00:00<00:00, 94.48it/s]\n",
            "100%|| 16/16 [00:00<00:00, 80.14it/s]\n",
            "100%|| 16/16 [00:00<00:00, 93.73it/s]\n",
            "100%|| 16/16 [00:00<00:00, 84.97it/s]\n",
            "100%|| 16/16 [00:00<00:00, 85.65it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.17it/s]\n",
            "100%|| 16/16 [00:00<00:00, 101.42it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.31it/s]\n",
            "100%|| 16/16 [00:00<00:00, 99.32it/s]\n",
            "100%|| 16/16 [00:00<00:00, 98.22it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.62it/s]\n",
            "100%|| 16/16 [00:00<00:00, 98.57it/s]\n",
            "100%|| 16/16 [00:00<00:00, 97.05it/s]\n",
            "100%|| 16/16 [00:00<00:00, 98.09it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.44it/s]\n",
            "100%|| 16/16 [00:00<00:00, 99.09it/s]\n",
            "100%|| 16/16 [00:00<00:00, 97.04it/s]\n",
            "100%|| 16/16 [00:00<00:00, 99.80it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.54it/s]\n",
            "100%|| 16/16 [00:00<00:00, 98.86it/s]\n",
            "100%|| 16/16 [00:00<00:00, 101.09it/s]\n",
            "100%|| 16/16 [00:00<00:00, 99.81it/s]\n",
            "100%|| 16/16 [00:00<00:00, 101.50it/s]\n",
            "100%|| 16/16 [00:00<00:00, 91.24it/s]\n",
            "100%|| 16/16 [00:00<00:00, 101.29it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.99it/s]\n",
            "100%|| 16/16 [00:00<00:00, 97.35it/s] \n",
            "100%|| 16/16 [00:00<00:00, 99.69it/s]\n",
            "100%|| 16/16 [00:00<00:00, 84.09it/s]\n",
            "100%|| 16/16 [00:00<00:00, 78.49it/s]\n",
            "100%|| 16/16 [00:00<00:00, 82.74it/s]\n",
            "100%|| 16/16 [00:00<00:00, 82.43it/s]\n",
            "100%|| 16/16 [00:00<00:00, 90.13it/s]\n",
            "100%|| 16/16 [00:00<00:00, 80.50it/s]\n",
            "100%|| 16/16 [00:00<00:00, 101.79it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.13it/s]\n",
            "100%|| 16/16 [00:00<00:00, 101.88it/s]\n",
            "100%|| 16/16 [00:00<00:00, 95.42it/s]\n",
            "100%|| 16/16 [00:00<00:00, 101.19it/s]\n",
            "100%|| 16/16 [00:00<00:00, 99.82it/s]\n",
            "100%|| 16/16 [00:00<00:00, 97.88it/s] \n",
            "100%|| 16/16 [00:00<00:00, 100.46it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.90it/s]\n",
            "100%|| 16/16 [00:00<00:00, 98.52it/s]\n",
            "100%|| 16/16 [00:00<00:00, 99.13it/s] \n",
            "100%|| 16/16 [00:00<00:00, 98.26it/s]\n",
            "100%|| 16/16 [00:00<00:00, 101.13it/s]\n",
            "100%|| 16/16 [00:00<00:00, 90.54it/s]\n",
            "100%|| 16/16 [00:00<00:00, 102.79it/s]\n",
            "100%|| 16/16 [00:00<00:00, 97.30it/s]\n",
            "100%|| 16/16 [00:00<00:00, 98.17it/s] \n",
            "100%|| 16/16 [00:00<00:00, 98.42it/s]\n",
            "100%|| 16/16 [00:00<00:00, 102.30it/s]\n",
            "100%|| 16/16 [00:00<00:00, 99.87it/s]\n",
            "100%|| 16/16 [00:00<00:00, 98.60it/s]\n",
            "100%|| 16/16 [00:00<00:00, 98.62it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.87it/s]\n",
            "100%|| 16/16 [00:00<00:00, 79.95it/s]\n",
            "100%|| 16/16 [00:00<00:00, 95.38it/s]\n",
            "100%|| 16/16 [00:00<00:00, 81.03it/s]\n",
            "100%|| 16/16 [00:00<00:00, 84.24it/s]\n",
            "100%|| 16/16 [00:00<00:00, 98.69it/s]\n",
            "100%|| 16/16 [00:00<00:00, 99.95it/s] \n",
            "100%|| 16/16 [00:00<00:00, 99.15it/s]\n",
            "100%|| 16/16 [00:00<00:00, 101.37it/s]\n",
            "100%|| 16/16 [00:00<00:00, 97.40it/s]\n",
            "100%|| 16/16 [00:00<00:00, 101.18it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.70it/s]\n",
            "100%|| 16/16 [00:00<00:00, 101.82it/s]\n",
            "100%|| 16/16 [00:00<00:00, 96.65it/s]\n",
            "100%|| 16/16 [00:00<00:00, 102.30it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.44it/s]\n",
            "100%|| 16/16 [00:00<00:00, 98.21it/s]\n",
            "100%|| 16/16 [00:00<00:00, 96.19it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.94it/s]\n",
            "100%|| 16/16 [00:00<00:00, 99.45it/s]\n",
            "100%|| 16/16 [00:00<00:00, 102.03it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.24it/s]\n",
            "100%|| 16/16 [00:00<00:00, 101.31it/s]\n",
            "100%|| 16/16 [00:00<00:00, 93.90it/s]\n",
            "100%|| 16/16 [00:00<00:00, 101.94it/s]\n",
            "100%|| 16/16 [00:00<00:00, 93.50it/s]\n",
            "100%|| 16/16 [00:00<00:00, 97.08it/s] \n",
            "100%|| 16/16 [00:00<00:00, 92.39it/s]\n",
            "100%|| 16/16 [00:00<00:00, 99.88it/s] \n",
            "100%|| 16/16 [00:00<00:00, 82.28it/s]\n",
            "100%|| 16/16 [00:00<00:00, 96.38it/s]\n",
            "100%|| 16/16 [00:00<00:00, 80.51it/s]\n",
            "100%|| 16/16 [00:00<00:00, 92.15it/s]\n",
            "100%|| 16/16 [00:00<00:00, 83.04it/s]\n",
            "100%|| 16/16 [00:00<00:00, 87.87it/s]\n",
            "100%|| 16/16 [00:00<00:00, 94.59it/s]\n",
            "100%|| 16/16 [00:00<00:00, 101.77it/s]\n",
            "100%|| 16/16 [00:00<00:00, 98.31it/s]\n",
            "100%|| 16/16 [00:00<00:00, 98.31it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.02it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.67it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.09it/s]\n",
            "100%|| 16/16 [00:00<00:00, 102.38it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.98it/s]\n",
            "100%|| 16/16 [00:00<00:00, 102.09it/s]\n",
            "100%|| 16/16 [00:00<00:00, 92.48it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.39it/s]\n",
            "100%|| 16/16 [00:00<00:00, 97.35it/s]\n",
            "100%|| 16/16 [00:00<00:00, 95.64it/s]\n",
            "100%|| 16/16 [00:00<00:00, 92.10it/s]\n",
            "100%|| 16/16 [00:00<00:00, 101.35it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.40it/s]\n",
            "100%|| 16/16 [00:00<00:00, 95.96it/s] \n",
            "100%|| 16/16 [00:00<00:00, 99.98it/s]\n",
            "100%|| 16/16 [00:00<00:00, 102.44it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.25it/s]\n",
            "100%|| 16/16 [00:00<00:00, 99.19it/s] \n",
            "100%|| 16/16 [00:00<00:00, 93.34it/s]\n",
            "100%|| 16/16 [00:00<00:00, 86.06it/s]\n",
            "100%|| 16/16 [00:00<00:00, 74.00it/s]\n",
            "100%|| 16/16 [00:00<00:00, 86.43it/s]\n",
            "100%|| 16/16 [00:00<00:00, 76.42it/s]\n",
            "100%|| 16/16 [00:00<00:00, 93.70it/s]\n",
            "100%|| 16/16 [00:00<00:00, 91.23it/s]\n",
            "100%|| 16/16 [00:00<00:00, 99.56it/s] \n",
            "100%|| 16/16 [00:00<00:00, 99.39it/s]\n",
            "100%|| 16/16 [00:00<00:00, 102.45it/s]\n",
            "100%|| 16/16 [00:00<00:00, 87.78it/s]\n",
            "100%|| 16/16 [00:00<00:00, 96.81it/s]\n",
            "100%|| 16/16 [00:00<00:00, 99.03it/s]\n",
            "100%|| 16/16 [00:00<00:00, 96.98it/s] \n",
            "100%|| 16/16 [00:00<00:00, 97.92it/s]\n",
            "100%|| 16/16 [00:00<00:00, 101.62it/s]\n",
            "100%|| 16/16 [00:00<00:00, 98.58it/s]\n",
            "100%|| 16/16 [00:00<00:00, 96.48it/s]\n",
            "100%|| 16/16 [00:00<00:00, 99.60it/s]\n",
            "100%|| 16/16 [00:00<00:00, 99.50it/s] \n",
            "100%|| 16/16 [00:00<00:00, 97.84it/s]\n",
            "100%|| 16/16 [00:00<00:00, 101.04it/s]\n",
            "100%|| 16/16 [00:00<00:00, 98.79it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.83it/s]\n",
            "100%|| 16/16 [00:00<00:00, 94.05it/s]\n",
            "100%|| 16/16 [00:00<00:00, 102.05it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.79it/s]\n",
            "100%|| 16/16 [00:00<00:00, 97.84it/s] \n",
            "100%|| 16/16 [00:00<00:00, 95.63it/s]\n",
            "100%|| 16/16 [00:00<00:00, 101.86it/s]\n",
            "100%|| 16/16 [00:00<00:00, 82.38it/s]\n",
            "100%|| 16/16 [00:00<00:00, 92.98it/s]\n",
            "100%|| 16/16 [00:00<00:00, 76.02it/s]\n",
            "100%|| 16/16 [00:00<00:00, 91.61it/s]\n",
            "100%|| 16/16 [00:00<00:00, 73.79it/s]\n",
            "100%|| 16/16 [00:00<00:00, 86.50it/s]\n",
            "100%|| 16/16 [00:00<00:00, 99.42it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.44it/s]\n",
            "100%|| 16/16 [00:00<00:00, 94.76it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.49it/s]\n",
            "100%|| 16/16 [00:00<00:00, 40.57it/s]\n",
            "100%|| 16/16 [00:00<00:00, 88.27it/s]\n",
            "100%|| 16/16 [00:00<00:00, 99.98it/s]\n",
            "100%|| 16/16 [00:00<00:00, 97.78it/s]\n",
            "100%|| 16/16 [00:00<00:00, 99.61it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.00it/s]\n",
            "100%|| 16/16 [00:00<00:00, 99.68it/s]\n",
            "100%|| 16/16 [00:00<00:00, 101.20it/s]\n",
            "100%|| 16/16 [00:00<00:00, 98.90it/s]\n",
            "100%|| 16/16 [00:00<00:00, 102.32it/s]\n",
            "100%|| 16/16 [00:00<00:00, 94.88it/s]\n",
            "100%|| 16/16 [00:00<00:00, 100.63it/s]\n",
            "100%|| 16/16 [00:00<00:00, 97.80it/s]\n",
            "100%|| 16/16 [00:00<00:00, 97.28it/s] \n",
            "100%|| 16/16 [00:00<00:00, 98.51it/s]\n",
            "100%|| 16/16 [00:00<00:00, 102.50it/s]\n",
            "100%|| 16/16 [00:00<00:00, 99.45it/s]\n",
            "100%|| 16/16 [00:00<00:00, 101.52it/s]\n",
            "100%|| 16/16 [00:00<00:00, 88.92it/s]\n",
            "100%|| 16/16 [00:00<00:00, 86.01it/s]\n",
            "100%|| 16/16 [00:00<00:00, 77.50it/s]\n",
            "100%|| 16/16 [00:00<00:00, 87.61it/s]\n",
            "100%|| 16/16 [00:00<00:00, 68.81it/s]\n",
            "100%|| 16/16 [00:00<00:00, 82.77it/s]\n",
            "100%|| 16/16 [00:00<00:00, 99.57it/s]\n",
            "100%|| 16/16 [00:00<00:00, 97.93it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "contrastivity_scores.to_csv(f\"/content/drive/MyDrive/clxai/results_contrast/test2/{model_version}_contrastivity_xai_scores.csv\", index=False)"
      ],
      "metadata": {
        "id": "Oxiz1NGNFN9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1 / 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "LjlZnUxqG4jx",
        "outputId": "4442af10-ae81-4517-f0af-27023011c145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "division by zero",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1455669704.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis"
      ],
      "metadata": {
        "id": "zqC9i83VbnK_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## faith"
      ],
      "metadata": {
        "id": "Rxe-0gywsQFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mode = 'irof'"
      ],
      "metadata": {
        "id": "rw2yrGonhbpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ce_path = f'/content/drive/MyDrive/clxai/results_faith/test_2/ce_{mode}_xai_scores.csv'\n",
        "scl_path = f'/content/drive/MyDrive/clxai/results_faith/test_2/scl_{mode}_xai_scores.csv'"
      ],
      "metadata": {
        "id": "pWLiCSCAL-DA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ce_df = pd.read_csv(ce_path)\n",
        "scl_df = pd.read_csv(scl_path)"
      ],
      "metadata": {
        "id": "iXJzpPJpb54r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'CE: {round((ce_df['true'] == ce_df['pred']).mean() * 100, 2)}')\n",
        "print(f'SCL: {round((scl_df['true'] == scl_df['pred']).mean() * 100, 2)}')"
      ],
      "metadata": {
        "id": "UQ5cHiZzcEvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask1 = ce_df['true'] == ce_df['pred']\n",
        "mask2 = scl_df['true'] == scl_df['pred']\n",
        "final_mask = mask1 & mask2\n",
        "ce_filtered = ce_df[final_mask].copy()\n",
        "scl_filtered = scl_df[final_mask].copy()\n",
        "ce_filtered = ce_filtered.reset_index(drop=True)\n",
        "scl_filtered = scl_filtered.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "EDE-WbLtGBkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scl_filtered['GradCAM_IROF_AOC'].mean()"
      ],
      "metadata": {
        "id": "RHGItBtRjTF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ce_filtered['GradCAM_IROF_AOC'].mean()"
      ],
      "metadata": {
        "id": "4aWktyzbje6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scl_filtered['AblationCAM_IROF_AOC'].mean()"
      ],
      "metadata": {
        "id": "MNYY0HEijgt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ce_filtered['AblationCAM_IROF_AOC'].mean()"
      ],
      "metadata": {
        "id": "WK6wClqgj2UG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scl_filtered['EigenCAM_IROF_AOC'].mean()"
      ],
      "metadata": {
        "id": "rmarSod5j9We"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ce_filtered['EigenCAM_IROF_AOC'].mean()"
      ],
      "metadata": {
        "id": "bAsBEpe3j-ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "sort_metric = \"GradCAM_PF_AUC\"\n",
        "diff_sort = ce_filtered[sort_metric] - scl_filtered[sort_metric]\n",
        "sorted_indices = diff_sort.sort_values().index\n",
        "\n",
        "metrics = [\"GradCAM_PF_AUC\", \"EigenCAM_PF_AUC\", \"AblationCAM_PF_AUC\"]\n",
        "\n",
        "for metric in metrics:\n",
        "    mean_ce = ce_filtered[metric].mean()\n",
        "    mean_scl = scl_filtered[metric].mean()\n",
        "\n",
        "    # reorder data\n",
        "    ce_values = ce_filtered.loc[sorted_indices, metric].values\n",
        "    scl_values = scl_filtered.loc[sorted_indices, metric].values\n",
        "\n",
        "    current_diff = ce_values - scl_values\n",
        "\n",
        "    plt.figure(figsize=(25, 10))\n",
        "    x_positions = np.arange(len(ce_values))\n",
        "\n",
        "    colors = ['#2ca02c' if d < 0 else '#d62728' for d in current_diff]\n",
        "\n",
        "    plt.vlines(x_positions, ymin=ce_values, ymax=scl_values,\n",
        "               colors=colors, linewidth=2, alpha=0.6, zorder=2)\n",
        "\n",
        "    # Plot the points\n",
        "    plt.scatter(x_positions, ce_values, color='royalblue', alpha=1.0,\n",
        "                label=f'ce_{mode}_df', s=50, zorder=4, edgecolors='white')\n",
        "    plt.scatter(x_positions, scl_values, color='darkorange', alpha=1.0,\n",
        "                label=f'scl_{mode}_df', s=50, zorder=4, edgecolors='white')\n",
        "\n",
        "    # Vertical grid boundaries between indices\n",
        "    boundary_positions = x_positions - 0.5\n",
        "    for x in boundary_positions[1:]:\n",
        "        plt.axvline(x=x, color='gray', linestyle='-', linewidth=0.5, alpha=0.2, zorder=1)\n",
        "\n",
        "    # Labels and Ticks\n",
        "    plt.xticks(ticks=x_positions, labels=sorted_indices, rotation=90, fontsize=12)\n",
        "    plt.xlim(-0.7, len(ce_values) - 0.3)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.3, zorder=1)\n",
        "\n",
        "    plt.title(f\"{metric}, mean CE: {mean_ce:0.3f} vs mean SCL: {mean_scl:0.3f}\",\n",
        "              fontsize=25, fontweight='bold', pad=20)\n",
        "    plt.xlabel(\"Original Sample ID\", fontsize=18)\n",
        "    plt.ylabel(f\"{metric} AUC\", fontsize=18)\n",
        "\n",
        "    plt.legend(loc='upper left', frameon=True, facecolor='white', framealpha=1, fontsize=15)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"/content/drive/MyDrive/clxai/results_faith/test_2/{mode}_diff_line_plot_{metric}_via_{sort_metric}.png\", bbox_inches='tight', dpi=300)\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "ST5snV9IdrGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "\n",
        "# # 1. Calculate the mean difference for each metric\n",
        "# # (ce_df - scl_df) -> Negative means ce_df is lower/better\n",
        "# metrics = [\"GradCAM_PF_AUC\", \"EigenCAM_PF_AUC\", \"HiResCAM_PF_AUC\"]\n",
        "# mean_diffs = [(ce_df[m] - scl_df[m]).mean() for m in metrics]\n",
        "\n",
        "# # 2. Setup colors based on improvement or regression\n",
        "# # Green for negative (improvement), Red for positive (regression)\n",
        "# bar_colors = ['#2ca02c' if diff < 0 else '#d62728' for diff in mean_diffs]\n",
        "\n",
        "# plt.figure(figsize=(10, 7))\n",
        "\n",
        "# # Create the bars\n",
        "# bars = plt.bar(metrics, mean_diffs, color=bar_colors, edgecolor='black', alpha=0.8)\n",
        "\n",
        "# # Add a horizontal line at 0 for reference\n",
        "# plt.axhline(0, color='black', linewidth=1.5, linestyle='-')\n",
        "\n",
        "# # 3. Add text labels on top/bottom of bars for the exact values\n",
        "# for bar in bars:\n",
        "#     height = bar.get_height()\n",
        "#     plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "#              f'{height:.4f}',\n",
        "#              ha='center', va='bottom' if height > 0 else 'top',\n",
        "#              fontsize=12, fontweight='bold')\n",
        "\n",
        "# # Formatting\n",
        "# plt.title(\"Average Difference in Faithfulness (ce_df - scl_df)\\nLower is Better (Negative = Improvement)\",\n",
        "#           fontsize=16, fontweight='bold', pad=20)\n",
        "# plt.ylabel(\"Mean AUC Difference\", fontsize=14)\n",
        "# plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
        "\n",
        "# # Add a slight buffer to the y-axis so labels don't get cut off\n",
        "# y_max = max(abs(min(mean_diffs)), abs(max(mean_diffs))) * 1.3\n",
        "# plt.ylim(-y_max, y_max)\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "6ccgn9v8g9ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sparse"
      ],
      "metadata": {
        "id": "_9j4Yjy8sTKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ce_complex_df = pd.read_csv('/content/drive/MyDrive/clxai/results_complex/test2/ce_sparse_xai_scores.csv')\n",
        "scl_complex_df = pd.read_csv('/content/drive/MyDrive/clxai/results_complex/test2/scl_sparse_xai_scores.csv')"
      ],
      "metadata": {
        "id": "4Rou-Am0sVAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask1 = ce_complex_df['true'] == ce_complex_df['pred']\n",
        "mask2 = scl_complex_df['true'] == scl_complex_df['pred']\n",
        "final_mask = mask1 & mask2\n",
        "ce_filtered = ce_complex_df[final_mask].copy()\n",
        "scl_filtered = scl_complex_df[final_mask].copy()\n",
        "ce_filtered = ce_filtered.reset_index(drop=True)\n",
        "scl_filtered = scl_filtered.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "3Kf9M_1ItP6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ce_filtered.mean()"
      ],
      "metadata": {
        "id": "NMq46oOtsg99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scl_filtered.mean()"
      ],
      "metadata": {
        "id": "Q8yUCsP9sh0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## complex"
      ],
      "metadata": {
        "id": "AtoPGl480-u6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ce_complex_df = pd.read_csv('/content/drive/MyDrive/clxai/results_complex/test2/ce_complex_xai_scores.csv')\n",
        "scl_complex_df = pd.read_csv('/content/drive/MyDrive/clxai/results_complex/test2/scl_complex_xai_scores.csv')"
      ],
      "metadata": {
        "id": "_KQHSe730wcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask1 = ce_complex_df['true'] == ce_complex_df['pred']\n",
        "mask2 = scl_complex_df['true'] == scl_complex_df['pred']\n",
        "final_mask = mask1 & mask2\n",
        "ce_filtered = ce_complex_df[final_mask].copy()\n",
        "scl_filtered = scl_complex_df[final_mask].copy()\n",
        "ce_filtered = ce_filtered.reset_index(drop=True)\n",
        "scl_filtered = scl_filtered.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "27G0SoO20wcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ce_filtered.mean()"
      ],
      "metadata": {
        "id": "E8nRB7hM0wcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scl_filtered.mean()"
      ],
      "metadata": {
        "id": "k_i7tyjD0wcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## contrastivity"
      ],
      "metadata": {
        "id": "AppPenkyKe53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ce_contrast_df = pd.read_csv('/content/drive/MyDrive/clxai/results_contrast/test2/ce_contrastivity_xai_scores.csv')\n",
        "scl_contrast_df = pd.read_csv('/content/drive/MyDrive/clxai/results_contrast/test2/scl_contrastivity_xai_scores.csv')"
      ],
      "metadata": {
        "id": "wiiIYgkz1GLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask1 = ce_contrast_df['true'] == ce_contrast_df['pred']\n",
        "mask2 = scl_contrast_df['true'] == scl_contrast_df['pred']\n",
        "final_mask = mask1 & mask2\n",
        "ce_filtered = ce_contrast_df[final_mask].copy()\n",
        "scl_filtered = scl_contrast_df[final_mask].copy()\n",
        "ce_filtered = ce_filtered.reset_index(drop=True)\n",
        "scl_filtered = scl_filtered.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "4dgEgQsvKwO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ce_filtered[['AblationCAM_L2_Contrastivity','AblationCAM_SSIM_Contrastivity','GradCAM_L2_Contrastivity','GradCAM_SSIM_Contrastivity']].mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "M0G2hzJ2LF-q",
        "outputId": "62d1c228-b397-4931-f80e-16c797309a0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AblationCAM_L2_Contrastivity       6.803687\n",
              "AblationCAM_SSIM_Contrastivity     0.594501\n",
              "GradCAM_L2_Contrastivity          18.148861\n",
              "GradCAM_SSIM_Contrastivity         0.008563\n",
              "dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>AblationCAM_L2_Contrastivity</th>\n",
              "      <td>6.803687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AblationCAM_SSIM_Contrastivity</th>\n",
              "      <td>0.594501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GradCAM_L2_Contrastivity</th>\n",
              "      <td>18.148861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GradCAM_SSIM_Contrastivity</th>\n",
              "      <td>0.008563</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 236
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scl_filtered[['AblationCAM_L2_Contrastivity','AblationCAM_SSIM_Contrastivity','GradCAM_L2_Contrastivity','GradCAM_SSIM_Contrastivity']].mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "e5EvH_1qLV3J",
        "outputId": "6bbd9054-e9f8-4d3a-c09f-60e0fa1acbad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AblationCAM_L2_Contrastivity       2.901425\n",
              "AblationCAM_SSIM_Contrastivity     0.858255\n",
              "GradCAM_L2_Contrastivity          12.921122\n",
              "GradCAM_SSIM_Contrastivity        -0.033921\n",
              "dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>AblationCAM_L2_Contrastivity</th>\n",
              "      <td>2.901425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AblationCAM_SSIM_Contrastivity</th>\n",
              "      <td>0.858255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GradCAM_L2_Contrastivity</th>\n",
              "      <td>12.921122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GradCAM_SSIM_Contrastivity</th>\n",
              "      <td>-0.033921</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 237
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## robustness"
      ],
      "metadata": {
        "id": "S9d4Fn6YNzfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ce_robust_df = pd.read_csv('/content/drive/MyDrive/clxai/results_contrast/test2/ce_robust_xai_scores.csv')\n",
        "scl_robust_df = pd.read_csv('/content/drive/MyDrive/clxai/results_contrast/test2/scl_robust_xai_scores.csv')"
      ],
      "metadata": {
        "id": "3eagCgArLZy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask1 = ce_robust_df['true'] == ce_robust_df['pred']\n",
        "mask2 = scl_robust_df['true'] == scl_robust_df['pred']\n",
        "final_mask = mask1 & mask2\n",
        "ce_filtered = ce_robust_df[final_mask].copy()\n",
        "scl_filtered = scl_robust_df[final_mask].copy()\n",
        "ce_filtered = ce_filtered.reset_index(drop=True)\n",
        "scl_filtered = scl_filtered.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "bzo33CyoN7_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ce_filtered.mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "2ScYl8UPOBFE",
        "outputId": "9d753471-ff8b-4424-964c-b79a3667c5cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "idx                            63.408696\n",
              "true                            4.895652\n",
              "pred                            4.895652\n",
              "GradCAM_SSIM_Robustness         0.513066\n",
              "EigenCAM_SSIM_Robustness        0.566275\n",
              "AblationCAM_SSIM_Robustness     0.647728\n",
              "dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>idx</th>\n",
              "      <td>63.408696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>true</th>\n",
              "      <td>4.895652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred</th>\n",
              "      <td>4.895652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GradCAM_SSIM_Robustness</th>\n",
              "      <td>0.513066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>EigenCAM_SSIM_Robustness</th>\n",
              "      <td>0.566275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AblationCAM_SSIM_Robustness</th>\n",
              "      <td>0.647728</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 241
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scl_filtered.mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "rcyzVYTMOCI3",
        "outputId": "8d3a5352-80af-4c10-d8f8-c663fccc82be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "idx                            63.408696\n",
              "true                            4.895652\n",
              "pred                            4.895652\n",
              "GradCAM_SSIM_Robustness         0.591083\n",
              "EigenCAM_SSIM_Robustness        0.613452\n",
              "AblationCAM_SSIM_Robustness     0.599957\n",
              "dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>idx</th>\n",
              "      <td>63.408696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>true</th>\n",
              "      <td>4.895652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred</th>\n",
              "      <td>4.895652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GradCAM_SSIM_Robustness</th>\n",
              "      <td>0.591083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>EigenCAM_SSIM_Robustness</th>\n",
              "      <td>0.613452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AblationCAM_SSIM_Robustness</th>\n",
              "      <td>0.599957</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 242
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1vBBwlBKOFxo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}